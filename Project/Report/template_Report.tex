\documentclass[]{report}

\usepackage[utf8]{inputenc}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage[square,sort,comma,numbers]{natbib}
\bibliographystyle{abbrv} 

% Title Page
\title{Solving multiple square jigsaw puzzles with missing pieces}
\author{Dominique Cheray and Manuel Krämer}

\begin{document}
\maketitle

\begin{abstract}
\end{abstract}

\tableofcontents

\chapter{Introduction}
\subsubsection*{by Dominique Cheray}
The task of solving a jigsaw puzzle is well known to children all around the
world. Given a set of often oddly shaped interlocking pieces whith small parts
of a picture on top of each piece one has to assemble the pieces in the correct
way to reconstruct the full image. But puzzling is not only a fun pastime, but
also finds application in areas such as archaeology \cite{brown2008system,
  liu2011automated, koller2006computer}, biology \cite{marande2007mitochondrial}, speech
descrambling \cite{zhao2007puzzle}, image editing \cite{cho2008patch} or the
reconstruction of document fragments \cite{zhu2008globally}. \\
The early automatic jigsaw puzzle solvers focus on solving the puzzle based
only on the shape of the pieces \cite{freeman1964apictorial, wolfson1988solving,
webster1990computer, kong2001solving}. Later on approaches are presented that
take into account not only the shape of the pieces but also their color
information \cite{kosiba1994automatic, makridis2006new, sagiroglu2006texture}.
Recent work began to focus on only using color information of the pieces
\cite{nielsen2008solving} which then eventually shifted to only considering jigsaw
puzzles with square pieces where color information is the only information that
can be used to find matching pieces \cite{Cho2010, yang2011particle,
  Pomeranz2011, gallagher2012jigsaw, son2014solving, sholomon2013genetic,
  Paikin2015}. \\
The first to introduce jigsaw puzzles with square pieces are Cho et al.
\cite{Cho2010}. They present a probabilistic solver that can handle puzzles
of up to 432 pieces given some apriori knowledge of the puzzle, namely its size and the
location of few so-called ``anchor-pieces'' that are placed at their correct
location before placing of the other pieces starts. A year later Yang et al.
\cite{yang2011particle} improve the results of Cho et al. by using a paricle filter
based solver. \\
Pomeranz et al. \cite{Pomeranz2011} are the first to introduce a fully
automatic jigsaw puzzle solver who is based on a greedy placer. Their solver
can handle puzzles of up to 3,300 pieces. Later on Gallagher et al.
\cite{gallagher2012jigsaw} use a greedy tree algorithm and generalized the work
of Pomeranz et al. to also handle pieces of unknown orientation and puzzles
whose dimensions are not known. Son et al. \cite{son2014solving} further
improve the work of Gallagher et al. in handling pieces of unknown orientation
by adding ``loop constraints''. \\
Sholomon et al. \cite{sholomon2013genetic}, in turn, instead of using a greedy
solver, present a genetic algorithm that is able to solve large square jigsaw
puzzles. \\
The solver presented by Paikin \&Tal \cite{Paikin2015}, which we reimplement for
this project, is inspired by the work of Pomeranz et al. \cite{Pomeranz2011}.
Their algorithm is also a greedy solver but is able to solve more challenging
puzzles than the solver proposed by Pomeranz et al. It can handle jigsaw puzzles with
missing pieces, pieces of unknown orientation, puzzles of unknown size and
multiple puzzles whose pieces are mixed together. As with previous solutions,
the placement of the puzzle pieces is based on a compatibility function, but
they provide a faster and more accurate function. In addition, since early
mistakes can have a fatal impact when using a greed placer, they take special
care in choosing the first piece. By contrast, earlier works randomly select the
first piece. Another change is that Paikin and Tal do not place puzzle pieces
absolutely, but in relation to the pieces already placed. So the next piece to
choose is not the piece that best fits a particular spot, but the one most
likely to be correct.\\
As part of this project, we will reimplement the puzzle solver presented by
Paikin and Tal. We test it with the same image datasets as the authors and
compare our results to theirs.


\chapter{Theoretical Background}
\section{Automatic Jigsaw Puzzle Solvers}
\subsubsection*{by Dominique Cheray}
The first jigsaw puzzles were produced around 1760 by John Spilsbury, a
London engraver and mapmaker, and made out of wood.
Hence the name ``jigsaw'' which refers to the jigsaws that were used to cut out
the pieces of the puzzles. Modern jigsaws puzzles, where an image was printed on
a cardboard sheet that was cut into a set of interlocking pieces were, introduced
in the 1930s \cite{williams2004jigsaw}. Even though puzzles are successfully
solved by children worldwide, automatic puzzle solvers are a technically
challenging problem. Demaine et al. \cite{demaine2007jigsaw} could show that the
puzzle problem is NP-complete if the pairwise affinity among pieces is
unreliable. \\
The first automatic jigsaw puzzle solver was proposed in 1964 by Freeman et al
\cite{freeman1964apictorial}. It was designed to solve jigsaw puzzles with
pieces which are all uniformly gray and the only available information is the
shape of the pieces and could handle up to nine-piece problems. Various other
early works explore aspects of using piece shape information and contour
matching to solve jigsaw puzzles. For example, Wolfson et al. \cite{wolfson1988solving}
use an approach that is also widely used by humans. They first reconstruct the boundary of
the jigsaw puzzle and then gradually fill the inside with the most reliably matching
pieces. Webster et al. \cite{webster1990computer} introduce a methodology that
derives a set of critical points which define a feature called an Isthmus. This
Isthmus can be used in matching boundaries of planar regions. The approach of
Kong et al. \cite{kong2001solving} consists of two steps. In the first step
local shape matching is used to find likely candidate pairs for adjacent
fragments. In the second step ambiguities resulting from local shape matching
are resolved by finding a global solution and the pieces are merged together.
They report successfully reassembling broken ceramic tiles and a map puzzle with
their approach. \\
The first to not only consider information about the shape of a piece but also
its color information were Kosiba et al. \cite{kosiba1994automatic}. The color
information is used differently depending on whether the original image is known
or not. If the original image is known they calculate the overall color
characteristic of each piece, meaning the mean and variance of the hue,
saturation and intensity values. They then compare these features with color
characteristics of various regions in the original image and try to reassemble
the jigsaw puzzle in the same orientation as the original image. If the
original image is unknown small color windows at regular intervals along the
border of the pieces are sampled and color characteristics for each of these
windows are calculated. These features are then, together with the shape
information of the pieces, used to compare the borders of pieces and find likely
matching pairs. Makridis et al. \cite{makridis2006new} consider a puzzle as an
image that is divided into a number of subimages (pieces). For each piece they
extract a set of boundary characteristic points and for each characteristic
point a set of color and geometrical features is extracted. Then the sets of
features are compared to decide whether two subimages match or not. Two matching
pieces are then merged together to form a new subimage and the algorithm
proceeds with the new subimage and the remaining subimages until either all
subimages are merged to one image or no more matching subimages can be found.
Instead of color information Sagiroglu et al. \cite{sagiroglu2006texture} use
textural features and geometrical constraints to determine matching pieces. They
propose a texture prediction algorithm which predicts pixel values in a band
outside the border of a piece. The textural features of this predicted band are
then correlated with the original pictorial specifications of possible
neighboring pieces. They aim to maximize the matching and continuity of the
texture while the geometrical constraints are satisfied. \\
Nielsen et al. \cite{nielsen2008solving} are the first to only use image
features to solve a jigsaw puzzle. They consider only a single-pixel wide
continuous strip for each edge of a piece. Two pieces are considered a likely
match if there is little to no gradient at their common edges. This approach
allows them to solve not only jigsaw puzzles with many similar or identically shaped
pieces, but also jigsaw puzzles that consist only of rectangular pieces. Cho et
al. \cite{Cho2010} carry on this idea and only look at puzzles that consist of
square pieces. They develop a graphical model to solve the jigsaw puzzles. The
puzzle is built around a set of ``anchor pieces'' whose position is fixed at the
correct location before the other pieces are placed. Fixing
as few as 4 to 6 pieces at their correct location is enough to solve puzzles of
over 400 pieces. Their dissimilarity based
compatibility function to quantify the pairwise distance between two pieces
became the basis of most future work. To determine how similar two pieces are
the sum-of-squared color differences along the abutting boundary is calculated.
Yang et al. \cite{yang2011particle} improved the results of Cho et al.
\cite{Cho2010} by using a particle filter. Additionally they do not assume any
prior knowledge on the image layout or beforehand correct placed pieces. \\
Pomeranz et al. \cite{Pomeranz2011} are the first to propose an automatic square
jigsaw puzzle solver which is based on a greedy placer and a novel prediction
based dissimilarity. Similar to Yang et al. \cite{yang2011particle} their solver
does not require prior knowledge about the image or clues about the pieces'
location. The only input to the solver are the pieces themselves, their
orientation and the puzzle dimensions. The greedy solver then solves the jigsaw
puzzle in several steps. First the compatibility function is calculated to
measure the affinity between pieces. Second the placement is executed. Given a
single piece or a partial constructed puzzle, the placer tries to find the
position of the remaining parts on the board. This is then followed by a
segmentation step. Given the placements of all pieces on the board from the
previous step this partial solution is divided into segments which are estimated
to be assembled correctly, disregarding their absolute location. In a forth step
shifting is performed. Given a set of puzzle segments these segments and
remaining individual part are relocated on the board such that a better
approximate solution is obtained. These steps are then repeated until the
evaluation of the best buddies metric, a metric to determine how likely
neighboring pieces are true neighbors, reaches a local maximum. Gallagher et al.
\cite{gallagher2012jigsaw} generalize the approach of Pomeranz et al.
\cite{Pomeranz2011} to also handle pieces of unknown orientation and puzzles
with no information about their dimensions by using a greedy tree algorithm.
Additionally they use a Mahalanobis-inspired jigsaw piece compatibility measure.
Son et al. \cite{son2014solving} extend the approach of Gallagher et al.
\cite{gallagher2012jigsaw} by adding loop constraints. They use a loop-based
strategy to reconstruct jigsaw puzzles from the local matching candidates. Their
algorithm seeks out and exploits loops as a form of outlier detection. 
\\
Sholomon et al. \cite{sholomon2013genetic}, in turn, introduce a genetic
algorithm as a strategy for piece placement. A genetic algorithm contains a
population of chromosomes which in this case are placements of all puzzle
pieces. They start with 1,000 chromosomes this means 1,000 random placements. In
each generation the population is evaluated using a fitness function which is
based on the pairwise compatibility of every pair of adjacent pieces. Then a new
population is produced by the selection of chromosomes and the crossover of
chromosome pairs. The probability with which a chromosome is chosen for either
crossover or directly becoming part of the next generation is directly
proportional to the value of its fitness function. \\
The approach of Paikin \& Tal \cite{Paikin2015}, which we implement in this
project, is inspired by the work of Pomeranz et al. \cite{Pomeranz2011} and also
uses a greedy algorithm. Similar to previous works the placement of the pieces
is based on the compatibility between pieces. But they propose a more accurate
and faster compatibility function that takes not only advantage of the
similarity between pieces but also takes into account the reliability of this
similarity. Since a greedy algorithm is extremely vulnerable to early errors
they take special care when selecting the first piece to place. Unlike previous
works, where the first piece is randomly selected, Paikin \& Tal require the first piece
to have distinctive borders and lie in a distinctive region. In addition, they
do not choose the best piece for a specific location, but that piece that
minimizes the likelihood of making a mistake, regardless of its position. By
this approach they are able to also solve jigsaw puzzles with additional
challenges like puzzles with missing pieces, puzzles of unknown size, puzzles
with unknown orientation of the pieces and multiple puzzles whose pieces are
mixed together and neither the size of the puzzles is known or information on
possibly missing pieces are given. Only the number of puzzles to solve is known. \\
A more detailed description of the puzzle solver by Paikin \& Tal will be
provided in the following chapter in which we elaborate on our implementation of
the solver.

\chapter{Materials and Methods}
\subsubsection*{by Manuel Krämer}
\section{Algorithm}
The algorithm we implemented is the one that was used in \cite{Paikin2015}. The whole procedure can be described by three main steps:
\begin{enumerate}
	\item Compatibility between pieces: This is done by calculating the dissimilarity between two pieces and the second best dissimilarity. With this score one can determine the so called "Best Buddies"-metric which indicates, that two pieces agree that each other is their best neighbor.
	\item First piece: A good piece to start with has best buddies in all four spatial relations and these four neighbors have best buddies in all directions as well.
	\item Placer: The placing of the pieces is done by grabbing the best piece (highest mutual compatibility) from a pool, placing this piece and adding the best buddies of it to the pool.
\end{enumerate}

\subsection{Compatibility between pieces}
The most fundamental part is the calculation of the compatibility between two pieces to determine how well they fit together. This is done by computing the dissimilarity between every pair of pieces and the confidence in this score.

First of all, the dissimilarity of two pieces $p_i$ and $p_j$ is defined as follows (see \cite{Paikin2015}): 
\begin{equation}\label{eq:dissimilarity}
D(p_i,p_j,right) = \sum_{k=1}^K \sum_{d=1}^3 ||((2p_i(k,K,d) - p_i(k,K-1,d)) - p_j(k,1,d) ||
\end{equation}
K is the piece size and d the dimension.

By just considering a small dissimilarity as a measure of adjacency the algorithm would face the problem, that this value is very high in distinctive regions. To deal with this fact we also calculate the dissimilarity of the second best neighbor; if these two values differ, the closest pieces are more likely to be neighbors. With that we can define the compatibility as proposed in \cite{Paikin2015}:
\begin{equation}\label{eq:compatibility}
C(p_i,p_j,r) = 1 - \frac{D(p_i,p_j,r)}{secondD(p_i,r)}
\end{equation}

This equation expresses how reliable a match of two pieces is. If now two pieces consider each other as their most likely neighbor in a certain spatial relation, these two are called best buddies.

\subsection{First piece}
One important task is to avoid early errors because the algorithm considers the unplaced and placed pieces as well. If there is a mistake within the first few pieces this can lead to following errors.
This means, we need a first piece that is located in a distinctive area where the probability of making errors is as low as possible. A good measure for this is the best buddies metric. We require the first piece to have best buddies in all four spatial relations, what ensures that the piece itself is distinctive. Furthermore, all four neighbors of this piece need to have best buddies in all four spatial relations as well, what means that the piece is located in a distinctive region. 
Usually there is more than one piece that satisfies these conditions. Out of all these distinctive pieces we choose the one with the best buddies that maximize the mutual compatibility in all four spatial relations (as defined in \cite{Paikin2015}):
\begin{equation}\label{eq:mutualComp}
\widetilde{C}(p_i,p_j,r_1) = \frac{C(p_i,p_j,r_1)+C(p_j,p_i,r_2)}{2}
\end{equation}

\subsection{Placer}
The placer algorithm as described in \cite{Paikin2015} places all pieces according to their mutual compatibility and maintains a pool with candidates to place. Since the description is not really detailed there are several possibilities to implement it. We want to give a more comprehensive explanation of our method (see algorithm \ref{algo:placer}).

\begin{algorithm}
	\caption{Placer algorithm}
	\label{algo:placer}
	\begin{algorithmic}
		\State Place the first piece into the pool
		\While{There are unplaced pieces}
			\While{Pool is not empty}
				\State Get the best item and its placing position from the pool
				\If{Placing position is occupied}
					\State Delete the piece from processedPieces and continue \EndIf
				\State Add this item to the placer List
				\State Delete this item from unplaced pieces
				\State Save the taken indices
				\State Calculate all best buddies and put them in the pool
			\EndWhile
		\Comment{Pool is empty}
		\State Get best piece of all unplaced
		\If{Placing position is not occupied}
			\State Put this piece on the pool
		\EndIf
		\EndWhile
	\end{algorithmic}
\end{algorithm}

With this approach we make sure that an already placed piece doesn't get overwritten by pieces that maybe have a high mutual probability as well. While processing through all pieces we keep track of the row and column indices of every piece so we can detect duplicates and, finally reconstruct the puzzled image.
\newpage

\section{Image data and evaluation}
By testing the algorithm we used images from \cite{Cho2010} (size: 432 pieces) and \cite{Pomeranz2011} (size: 540, 805 and 2360 pieces) to get comparable results. A small sample of these data one can see in fig. \ref{fig:database}

\begin{figure}
	\centering
	\begin{subfigure}[b]{0.45\textwidth}
		\includegraphics[width=\textwidth]{../imData/432/1.png}
		\caption{One image of the dataset we used for the puzzle with 432 pieces}
		\label{img:432}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.45\textwidth}
		\includegraphics[width=\textwidth]{../imData/540/1.jpg}
		\caption{One image of the dataset we used for the puzzle with 504 pieces}
		\label{img:540}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.45\textwidth}
		\includegraphics[width=\textwidth]{../imData/805/1.jpg}
		\caption{One image of the dataset we used for the puzzle with 805 pieces}
		\label{img:805}
	\end{subfigure}
	~
	\begin{subfigure}[b]{0.45\textwidth}
		\includegraphics[width=\textwidth]{../imData/2360/1.jpg}
		\caption{One image of the dataset we used for the puzzle with 2360 pieces}
		\label{img:2360}
	\end{subfigure}

	\caption{A sample of all used databases}
	\label{fig:database}
\end{figure}

To evaluate our results we used the direct measure, i.e. to check the absolute position of every piece in the image.


\chapter{Results}

\chapter{Discussion}

\nocite{*}


\bibliography{literature}

\end{document}          
